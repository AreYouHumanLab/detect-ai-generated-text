{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.33.2\n",
      "0.5.0\n",
      "2.1.1+cu118\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import time, sys, gc, logging, random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType # type: ignore\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "from transformers import AutoTokenizer, LlamaForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import DataCollatorWithPadding\n",
    "import transformers\n",
    "import peft\n",
    "from accelerate import Accelerator\n",
    "import bitsandbytes\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from shutil import rmtree\n",
    "import language_tool_python\n",
    "# import optuna\n",
    "import concurrent\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from concurrent.futures import wait\n",
    "\n",
    "print(transformers.__version__)\n",
    "print(peft.__version__)\n",
    "print(torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_tool = language_tool_python.LanguageTool('en-US')\n",
    "N_FOLD = 5\n",
    "SEED = 42\n",
    "DEBUG = True\n",
    "IS_TRAIN = True\n",
    "\n",
    "# Seed the same seed to all \n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "seed_everything()\n",
    "# Create new `pandas` methods which use `tqdm` progress\n",
    "# (can use tqdm_gui, optional kwargs, etc.)\n",
    "tqdm.pandas()\n",
    "\n",
    "log_level = \"DEBUG\"\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    "    level=logging.WARNING\n",
    ")\n",
    "\n",
    "# set the main code and the modules it uses to the same log-level according to the node\n",
    "transformers.utils.logging.set_verbosity(log_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validation\n",
    "def cv_split(train_data):\n",
    "    skf = StratifiedKFold(n_splits=N_FOLD, shuffle=True, random_state=SEED)\n",
    "    X = train_data.loc[:, train_data.columns != \"label\"]\n",
    "    y = train_data.loc[:, train_data.columns == \"label\"]\n",
    "\n",
    "    for fold, (train_index, valid_index) in enumerate(skf.split(X, y)):\n",
    "        train_data.loc[valid_index, \"fold\"] = fold\n",
    "\n",
    "    print(train_data.groupby(\"fold\")[\"label\"].value_counts())\n",
    "    display(train_data.head())\n",
    "    return train_data\n",
    "\n",
    "def pre_processing_text(text):\n",
    "    text = text.replace('\\n', ' ')\n",
    "    typos = language_tool.check(text) # typo is a list\n",
    "    # Check how many typos\n",
    "    #if len(typos) > 0:\n",
    "    #print(f\"The number of typos = {len(typos)}\\n {typos}\")\n",
    "    text = language_tool.correct(text)\n",
    "    return text\n",
    "\n",
    "# Run pre-processing texts in parallel\n",
    "def parallel_pre_processing_text(texts):\n",
    "    print(f\"Total number of texts {len(texts)}\")\n",
    "    results = []\n",
    "    # run 'pre_processing' fucntions in the process pool\n",
    "    with ThreadPoolExecutor(4) as executor:\n",
    "        # results = list(tqdm(executor.map(pre_processing_text, texts)))\n",
    "        # send in the tasks\n",
    "        futures = [executor.submit(pre_processing_text, text) for text in texts]\n",
    "        # wait for all tasks to complete\n",
    "        for future in futures:\n",
    "            results.append(future.result())\n",
    "            if len(results) % 100 == 0:\n",
    "                print(f\"Finished {len(results)} / {len(texts)}\\n\", end='', flush=True)\n",
    "    # wait for all tasks to complete\n",
    "    print(\"results\", len(results))\n",
    "    return results\n",
    "    \n",
    "    \n",
    "def load_train_data():\n",
    "    train_df = pd.read_csv(\"/root/03-S_NLP/DetectAI/00-data/train_essays.csv\", sep=',')\n",
    "    train_prompts_df = pd.read_csv(\"/root/03-S_NLP/DetectAI/00-data/train_prompts.csv\", sep=',')\n",
    "\n",
    "    # rename column generated to label and remove used 'id' and 'prompt_id' columns\n",
    "    # Label: 1 indicates generated texts (by LLMs) \n",
    "    train_df = train_df.rename(columns={'generated': 'label'})\n",
    "    train_df = train_df.reset_index(drop=True)\n",
    "    train_df = train_df.drop(['id', 'prompt_id'], axis=1)\n",
    "#     print(\"Start processing training data's text\")\n",
    "#     start = time.time()\n",
    "#     # Clear text in both train and test dataset\n",
    "#     train_df['text'] = train_df['text'].progress_apply(lambda text: pre_processing_text(text))\n",
    "#     display(train_df.head())\n",
    "#     print(f\"Correct the training data's texts with {time.time() - start : .1f} seconds\")\n",
    "    \n",
    "    # Include external data\n",
    "    external_df = pd.read_csv(\"/root/03-S_NLP/DetectAI/00-data/train_v2_drcat_02.csv\", sep=',')\n",
    "    # We only need 'text' and 'label' columns\n",
    "    external_df = external_df[[\"text\", \"label\"]]\n",
    "    external_df[\"label\"] = 1\n",
    "    \n",
    "#     print(\"Start processing external data's texts\")\n",
    "#     start = time.time()\n",
    "#     external_df['text'] = parallel_pre_processing_text(external_df['text'].to_list())\n",
    "#     print(f\"Correct the external data's texts with {time.time() - start : .1f} seconds\")\n",
    "#     # external_df['text'] = external_df['text'].map(lambda text: pre_processing_text(text))\n",
    "#     display(external_df.head())\n",
    "#     external_df.to_csv('train_v2_drcat_02_fixed.csv', index=False)\n",
    "    # Merge train and external data into train_data\n",
    "    train_data = pd.concat([train_df, external_df])\n",
    "    train_data.reset_index(inplace=True, drop=True)\n",
    "    # print(f\"Train data has shape: {train_data.shape}\")\n",
    "    print(f\"Train data {train_data.value_counts('label')}\") # 1: generated texts 0: human texts\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data label\n",
      "1    44871\n",
      "0     1375\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cars. Cars have been around since they became ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Transportation is a large necessity in most co...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"America's love affair with it's vehicles seem...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How often do you ride in a car? Do you drive a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cars are a wonderful thing. They are perhaps o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46241</th>\n",
       "      <td>Dear Senator,\\n\\nI am writing to you today to ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46242</th>\n",
       "      <td>Dear Senator,\\n\\nI am writing to you today to ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46243</th>\n",
       "      <td>Dear Senator,\\n\\nI am writing to you today to ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46244</th>\n",
       "      <td>Dear Senator,\\n\\nI am writing to you today to ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46245</th>\n",
       "      <td>Dear Senator,\\n\\nI am writing to you today to ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46246 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "0      Cars. Cars have been around since they became ...      0\n",
       "1      Transportation is a large necessity in most co...      0\n",
       "2      \"America's love affair with it's vehicles seem...      0\n",
       "3      How often do you ride in a car? Do you drive a...      0\n",
       "4      Cars are a wonderful thing. They are perhaps o...      0\n",
       "...                                                  ...    ...\n",
       "46241  Dear Senator,\\n\\nI am writing to you today to ...      1\n",
       "46242  Dear Senator,\\n\\nI am writing to you today to ...      1\n",
       "46243  Dear Senator,\\n\\nI am writing to you today to ...      1\n",
       "46244  Dear Senator,\\n\\nI am writing to you today to ...      1\n",
       "46245  Dear Senator,\\n\\nI am writing to you today to ...      1\n",
       "\n",
       "[46246 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_train_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrained model and add an extra layer with PEFT library for fine-tuning\n",
    "def load_model(fold):\n",
    "    TARGET_MODEL = \"mistralai/Mistral-7B-v0.1\"\n",
    "    # TARGET_MODEL = \"/kaggle/input/mistral-7b-v0-1/Mistral-7B-v0.1\"\n",
    "    # LoRA: Low-Rank Adaptation of Large Language Models\n",
    "    peft_config = LoraConfig(\n",
    "        r=64,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.SEQ_CLS,\n",
    "        inference_mode=False,\n",
    "        target_modules=[\n",
    "            \"q_proj\",\n",
    "            \"v_proj\"\n",
    "        ],\n",
    "    )\n",
    "    # Enable GPU to run the model with 4bit\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "    # Load the tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(TARGET_MODEL, use_fast=False)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    # Load the model\n",
    "    base_model = LlamaForSequenceClassification.from_pretrained(TARGET_MODEL,\n",
    "                                                                num_labels=2, # label is 0 or 1\n",
    "                                                                quantization_config=bnb_config,                                                                 \n",
    "                                                                device_map=\"auto\")\n",
    "    base_model.config.pretraining_tp = 1 # 1 is 7b\n",
    "    base_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    \n",
    "    if IS_TRAIN:\n",
    "        # Parameter-Efficient Fine-Tuning (PEFT) methods enable efficient adaptation of \n",
    "        # pre-trained language models (PLMs) to various downstream applications \n",
    "        # without fine-tuning all the model's parameters. \n",
    "        # https://github.com/huggingface/peft\n",
    "        model = get_peft_model(base_model, peft_config)\n",
    "    else:\n",
    "        OUTPUT_DIR = f\"{fold}\"\n",
    "        # OUTPUT_DIR = f\"/kaggle/working/mistral_7b_fold{fold}\"\n",
    "        # Load the pretrained model with PEFT\n",
    "        model = PeftModel.from_pretrained(base_model, str(OUTPUT_DIR))\n",
    "    \n",
    "    model.print_trainable_parameters() # Display the trainable parameters\n",
    "    \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples, tokenizer, max_length=512):\n",
    "    examples[\"text\"] = list(map(lambda text: pre_processing_text(text), examples[\"text\"]))\n",
    "    return tokenizer(examples[\"text\"], truncation=True, max_length=max_length, padding=True)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    accuracy_val = accuracy_score(labels, predictions)\n",
    "    roc_auc_val = roc_auc_score(labels, predictions)\n",
    "    r = { \"accuracy\": accuracy_val,\n",
    "          \"roc_auc\": roc_auc_val}\n",
    "    # logging.debug(f'{r}')\n",
    "    return r\n",
    "\n",
    "\n",
    "def train_model_by_fold(fold):\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(f\"Start training the fold {fold} model\")\n",
    "    # Create train and valid dataset for a fold\n",
    "    fold_valid_df = train_data[train_data[\"fold\"] == fold]\n",
    "    fold_train_df = train_data[train_data[\"fold\"] != fold]\n",
    "    # Train the model with small (for debugging) or large samples\n",
    "    if DEBUG:\n",
    "        fold_train_df = fold_train_df.sample(frac =.05, random_state=SEED)\n",
    "        fold_valid_df = fold_valid_df.sample(frac =.05, random_state=SEED)\n",
    "    else:\n",
    "        fold_train_df = fold_train_df.sample(frac =.3, random_state=SEED)\n",
    "        fold_valid_df = fold_valid_df.sample(frac =.3, random_state=SEED)\n",
    "\n",
    "    print(f'fold_train_df {fold_train_df.groupby(\"fold\")[\"label\"].value_counts()}')\n",
    "    print(f'fold_valid_df {fold_valid_df.groupby(\"fold\")[\"label\"].value_counts()}')\n",
    "    # create the dataset\n",
    "    train_ds = Dataset.from_pandas(fold_train_df)\n",
    "    valid_ds = Dataset.from_pandas(fold_valid_df)\n",
    "\n",
    "    # Load the pretrained model and tokenizer\n",
    "    model, tokenizer = load_model(fold)\n",
    "\n",
    "    # Tokenize the train and valid dataset and pass tokenizer as function argument\n",
    "    train_tokenized_ds = train_ds.map(preprocess_function, batched=True,\n",
    "                                      fn_kwargs={\"tokenizer\": tokenizer})\n",
    "    valid_tokenized_ds = valid_ds.map(preprocess_function, batched=True,\n",
    "                                      fn_kwargs={\"tokenizer\": tokenizer})\n",
    "    # Create data collator with padding (padding to the longest sequence)\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=\"longest\")\n",
    "\n",
    "    # Start training processing        \n",
    "    TMP_DIR = Path(f\"{fold}/\")\n",
    "    TMP_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    STEPS = 5 if DEBUG else 20\n",
    "    EPOCHS = 1 if DEBUG else 10\n",
    "    BATCH_SIZE = 2\n",
    "    training_args = TrainingArguments(output_dir=TMP_DIR,\n",
    "                                      learning_rate=5e-5,\n",
    "                                      per_device_train_batch_size=BATCH_SIZE,\n",
    "                                      per_device_eval_batch_size=1,\n",
    "                                      gradient_accumulation_steps=16,\n",
    "                                      max_grad_norm=0.3,\n",
    "                                      optim='paged_adamw_32bit',\n",
    "                                      lr_scheduler_type=\"cosine\",\n",
    "                                      num_train_epochs=EPOCHS,\n",
    "                                      weight_decay=0.01,\n",
    "                                      evaluation_strategy=\"epoch\",\n",
    "                                      save_strategy=\"epoch\",\n",
    "                                      load_best_model_at_end=True,\n",
    "                                      push_to_hub=False,\n",
    "                                      warmup_steps=STEPS,\n",
    "                                      eval_steps=STEPS,\n",
    "                                      logging_steps=STEPS,\n",
    "                                      report_to='none', # if DEBUG else 'wandb'\n",
    "                                      log_level='warning', # 'warning' is default level \n",
    "                                     )\n",
    "\n",
    "\n",
    "    # Create the trainer \n",
    "    trainer = Trainer(model=model,\n",
    "                      args=training_args,\n",
    "                      train_dataset=train_tokenized_ds,\n",
    "                      eval_dataset=valid_tokenized_ds,\n",
    "                      tokenizer=tokenizer,\n",
    "                      data_collator=data_collator,\n",
    "                      compute_metrics=compute_metrics)\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    OUTPUT_DIR = Path(f\"{fold}/\")\n",
    "    OUTPUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "    # Save the fine-tuned model\n",
    "    trainer.save_model(output_dir=str(OUTPUT_DIR))\n",
    "    print(f\"=== Finish the training for fold {fold} ===\")\n",
    "    del model, trainer, tokenizer\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data label\n",
      "1    44871\n",
      "0     1375\n",
      "Name: count, dtype: int64\n",
      "fold  label\n",
      "0.0   1        8975\n",
      "      0         275\n",
      "1.0   1        8974\n",
      "      0         275\n",
      "2.0   1        8974\n",
      "      0         275\n",
      "3.0   1        8974\n",
      "      0         275\n",
      "4.0   1        8974\n",
      "      0         275\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cars. Cars have been around since they became ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Transportation is a large necessity in most co...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"America's love affair with it's vehicles seem...</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How often do you ride in a car? Do you drive a...</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cars are a wonderful thing. They are perhaps o...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  fold\n",
       "0  Cars. Cars have been around since they became ...      0   1.0\n",
       "1  Transportation is a large necessity in most co...      0   0.0\n",
       "2  \"America's love affair with it's vehicles seem...      0   3.0\n",
       "3  How often do you ride in a car? Do you drive a...      0   2.0\n",
       "4  Cars are a wonderful thing. They are perhaps o...      0   0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training the fold 0 model\n",
      "fold_train_df fold  label\n",
      "1.0   1        464\n",
      "      0         18\n",
      "2.0   1        457\n",
      "      0         14\n",
      "3.0   1        449\n",
      "      0          9\n",
      "4.0   1        424\n",
      "      0         15\n",
      "Name: count, dtype: int64\n",
      "fold_valid_df fold  label\n",
      "0.0   1        446\n",
      "      0         16\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-v0.1/snapshots/5e9c98b96d071dce59368012254c55b0ec6f8658/tokenizer.model\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-v0.1/snapshots/5e9c98b96d071dce59368012254c55b0ec6f8658/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-v0.1/snapshots/5e9c98b96d071dce59368012254c55b0ec6f8658/tokenizer_config.json\n",
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass torch_dtype=torch.float16 to remove this warning.\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-v0.1/snapshots/5e9c98b96d071dce59368012254c55b0ec6f8658/config.json\n",
      "You are using a model of type mistral to instantiate a model of type llama. This is not supported for all configurations of models and can yield errors.\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 4096,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.33.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-v0.1/snapshots/5e9c98b96d071dce59368012254c55b0ec6f8658/pytorch_model.bin.index.json\n",
      "Instantiating LlamaForSequenceClassification model under default dtype torch.float16.\n",
      "Detected 4-bit loading: activating 4-bit loading for this model\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  7.05s/it]\n",
      "Some weights of the model checkpoint at mistralai/Mistral-7B-v0.1 were not used when initializing LlamaForSequenceClassification: ['lm_head.weight']\n",
      "- This IS expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at mistralai/Mistral-7B-v0.1 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 27,279,360 || all params: 7,137,939,456 || trainable%: 0.38217415779661107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1850/1850 [09:51<00:00,  3.13 examples/s]\n",
      "Map: 100%|██████████| 462/462 [02:30<00:00,  3.07 examples/s]\n",
      "Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!\n",
      "PyTorch: setting up devices\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='57' max='57' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [57/57 58:39, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Roc Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.771700</td>\n",
       "      <td>0.712891</td>\n",
       "      <td>0.958874</td>\n",
       "      <td>0.496637</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Finish the training for fold 0 ===\n",
      "Start training the fold 1 model\n",
      "fold_train_df fold  label\n",
      "0.0   1        434\n",
      "      0         19\n",
      "2.0   1        449\n",
      "      0         14\n",
      "3.0   1        439\n",
      "      0         11\n",
      "4.0   1        472\n",
      "      0         12\n",
      "Name: count, dtype: int64\n",
      "fold_valid_df fold  label\n",
      "1.0   1        447\n",
      "      0         15\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type mistral to instantiate a model of type llama. This is not supported for all configurations of models and can yield errors.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  7.30s/it]\n",
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at mistralai/Mistral-7B-v0.1 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 27,279,360 || all params: 7,137,939,456 || trainable%: 0.38217415779661107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1850/1850 [09:34<00:00,  3.22 examples/s]\n",
      "Map: 100%|██████████| 462/462 [02:23<00:00,  3.21 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='57' max='57' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [57/57 58:38, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Roc Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.676400</td>\n",
       "      <td>0.616211</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>0.525503</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Finish the training for fold 1 ===\n",
      "Start training the fold 2 model\n",
      "fold_train_df fold  label\n",
      "0.0   1        443\n",
      "      0         14\n",
      "1.0   1        429\n",
      "      0         16\n",
      "3.0   1        469\n",
      "      0         18\n",
      "4.0   1        453\n",
      "      0          8\n",
      "Name: count, dtype: int64\n",
      "fold_valid_df fold  label\n",
      "2.0   1        447\n",
      "      0         15\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type mistral to instantiate a model of type llama. This is not supported for all configurations of models and can yield errors.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  7.34s/it]\n",
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at mistralai/Mistral-7B-v0.1 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 27,279,360 || all params: 7,137,939,456 || trainable%: 0.38217415779661107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1850/1850 [09:35<00:00,  3.21 examples/s]\n",
      "Map: 100%|██████████| 462/462 [02:23<00:00,  3.21 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='57' max='57' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [57/57 58:37, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Roc Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.828300</td>\n",
       "      <td>0.692871</td>\n",
       "      <td>0.956710</td>\n",
       "      <td>0.494407</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Finish the training for fold 2 ===\n",
      "Start training the fold 3 model\n",
      "fold_train_df fold  label\n",
      "0.0   1        437\n",
      "      0         11\n",
      "1.0   1        474\n",
      "      0         14\n",
      "2.0   1        441\n",
      "      0         14\n",
      "4.0   1        442\n",
      "      0         17\n",
      "Name: count, dtype: int64\n",
      "fold_valid_df fold  label\n",
      "3.0   1        447\n",
      "      0         15\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type mistral to instantiate a model of type llama. This is not supported for all configurations of models and can yield errors.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  7.19s/it]\n",
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at mistralai/Mistral-7B-v0.1 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 27,279,360 || all params: 7,137,939,456 || trainable%: 0.38217415779661107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1850/1850 [09:34<00:00,  3.22 examples/s]\n",
      "Map: 100%|██████████| 462/462 [02:19<00:00,  3.31 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='57' max='57' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [57/57 58:38, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Roc Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.531800</td>\n",
       "      <td>0.646973</td>\n",
       "      <td>0.956710</td>\n",
       "      <td>0.494407</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Finish the training for fold 3 ===\n",
      "Start training the fold 4 model\n",
      "fold_train_df fold  label\n",
      "0.0   1        424\n",
      "      0         14\n",
      "1.0   1        465\n",
      "      0         13\n",
      "2.0   1        444\n",
      "      0         12\n",
      "3.0   1        461\n",
      "      0         17\n",
      "Name: count, dtype: int64\n",
      "fold_valid_df fold  label\n",
      "4.0   1        447\n",
      "      0         15\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type mistral to instantiate a model of type llama. This is not supported for all configurations of models and can yield errors.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.94s/it]\n",
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at mistralai/Mistral-7B-v0.1 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 27,279,360 || all params: 7,137,939,456 || trainable%: 0.38217415779661107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1850/1850 [09:31<00:00,  3.24 examples/s]\n",
      "Map: 100%|██████████| 462/462 [02:17<00:00,  3.36 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='57' max='57' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [57/57 58:37, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Roc Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.737900</td>\n",
       "      <td>0.556152</td>\n",
       "      <td>0.948052</td>\n",
       "      <td>0.522148</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Finish the training for fold 4 ===\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "Training time of fold 0 =  21579.4 seconds",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m Training time of fold 0 =  21579.4 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3516: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# # Check if we need to fine-tune the LLM model\n",
    "if IS_TRAIN:\n",
    "    start = time.time()\n",
    "    # Load train data\n",
    "    train_data = load_train_data()\n",
    "    # Cross validation with 5 fold\n",
    "    train_data = cv_split(train_data)\n",
    "    # Train the model  \n",
    "    # fold = 0\n",
    "    for fold in range(N_FOLD):\n",
    "        train_model_by_fold(fold)\n",
    "    #     # Add multiple threads to run each fold model concurrently\n",
    "\n",
    "    #with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:\n",
    "   #     futures = [executor.submit(train_model_by_fold, fold) for fold in range(2)]\n",
    "   #     # wait for all tasks to complete\n",
    "    #    wait(futures)\n",
    "    #    print('All training tasks are done!')\n",
    "    \n",
    "    for idx, fold in enumerate(range(N_FOLD)):\n",
    "        sys.exit(f\"Training time of fold {fold} = {time.time() - start: .1f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  5.05it/s]\n"
     ]
    }
   ],
   "source": [
    "import concurrent\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from concurrent.futures import wait\n",
    "from scipy.special import expit as sigmoid\n",
    "# Load test data\n",
    "test_df = pd.read_csv(\"/root/03-S_NLP/DetectAI/00-data/test_essays.csv\", sep=',')\n",
    "test_df = test_df.rename(columns={'generated': 'label'})\n",
    "test_df['text'] = test_df['text'].progress_apply(lambda text: pre_processing_text(text))\n",
    "# print(f'test_df.shape: {test_df.shape}')\n",
    "# test_df.head(3)\n",
    "\n",
    "def clear_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "# Sigmoid activation function can map 'x' between 0 and 1\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x)) \n",
    "\n",
    "def predict_result_by_fold(fold):\n",
    "    clear_memory()\n",
    "    print(f\"=== Start prediction with {fold} ===\")\n",
    "    model, tokenizer = load_model(fold) \n",
    "    # Load the test dataframe as dataset\n",
    "    test_ds = Dataset.from_pandas(test_df)\n",
    "    test_tokenized_ds = test_ds.map(preprocess_function, batched=True,\n",
    "                                    fn_kwargs={\"tokenizer\": tokenizer})\n",
    "    # Data collator\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer, \n",
    "                                            padding=\"longest\")\n",
    "    # Create the trainer\n",
    "    trainer = Trainer(model=model,\n",
    "                      tokenizer=tokenizer,\n",
    "                      data_collator=data_collator)\n",
    "    pred_output = trainer.predict(test_tokenized_ds)\n",
    "    logits = pred_output.predictions\n",
    "    # Apply sigmoid to \n",
    "    probs = sigmoid(logits[:, 1])\n",
    "    print(f\"fold = {fold} probs = {probs}\")\n",
    "    global predictions\n",
    "    for i, prob in enumerate(probs):\n",
    "        predictions[i].append(prob)  \n",
    "    # Clear memory\n",
    "    del model, trainer, tokenizer, test_ds, test_tokenized_ds, data_collator\n",
    "    clear_memory()\n",
    "    \n",
    "def predict_result():\n",
    "    global predictions\n",
    "    predictions = [[] for i in range(len(test_df))]\n",
    "    start = time.time()\n",
    "    print(f\"=== Begin prediction  ===\")\n",
    "\n",
    "    for fold in range(N_FOLD):\n",
    "       predict_result_by_fold(fold)\n",
    "    # fold = 0\n",
    "    # predict_result_by_fold(fold)\n",
    "    print(f\"Finish prediction in {time.time() - start: .1f} seconds\")\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Begin prediction  ===\n",
      "=== Start prediction with 0 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type mistral to instantiate a model of type llama. This is not supported for all configurations of models and can yield errors.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.89s/it]\n",
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at mistralai/Mistral-7B-v0.1 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 27,279,360 || all params: 7,137,939,456 || trainable%: 0.38217415779661107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3/3 [00:00<00:00, 16.57 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold = 0 probs = [1. 1. 1.]\n",
      "=== Start prediction with 1 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type mistral to instantiate a model of type llama. This is not supported for all configurations of models and can yield errors.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  7.24s/it]\n",
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at mistralai/Mistral-7B-v0.1 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 27,279,360 || all params: 7,137,939,456 || trainable%: 0.38217415779661107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3/3 [00:00<00:00, 14.98 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold = 1 probs = [0.02785   0.0001465 0.0004637]\n",
      "=== Start prediction with 2 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type mistral to instantiate a model of type llama. This is not supported for all configurations of models and can yield errors.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  7.12s/it]\n",
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at mistralai/Mistral-7B-v0.1 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 27,279,360 || all params: 7,137,939,456 || trainable%: 0.38217415779661107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3/3 [00:00<00:00, 15.56 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold = 2 probs = [0.02785   0.0001465 0.0004637]\n",
      "=== Start prediction with 3 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type mistral to instantiate a model of type llama. This is not supported for all configurations of models and can yield errors.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  7.05s/it]\n",
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at mistralai/Mistral-7B-v0.1 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 27,279,360 || all params: 7,137,939,456 || trainable%: 0.38217415779661107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3/3 [00:00<00:00, 15.57 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold = 3 probs = [0.02785   0.0001465 0.0004637]\n",
      "=== Start prediction with 4 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type mistral to instantiate a model of type llama. This is not supported for all configurations of models and can yield errors.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  7.09s/it]\n",
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at mistralai/Mistral-7B-v0.1 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 27,279,360 || all params: 7,137,939,456 || trainable%: 0.38217415779661107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3/3 [00:00<00:00, 16.92 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold = 4 probs = [0.02785   0.0001465 0.0004637]\n",
      "Finish prediction in  111.2 seconds\n",
      "[0.2223, 0.2001, 0.2003]\n"
     ]
    }
   ],
   "source": [
    "predictions = predict_result()\n",
    "probs = [np.mean(pred) for pred in predictions] \n",
    "print(probs)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_memory()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
